"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7302],{493:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});var a=r(4848),i=r(8453);const s={title:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",description:"Weeks 8-10 - NVIDIA Isaac Sim, Isaac ROS, and Nav2 for perception and path planning in humanoid robotics",tags:["NVIDIA Isaac","Isaac Sim","Isaac ROS","Nav2","Path Planning","Humanoid Robotics","Perception"]},o="Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",t={id:"week-8-10/week-8-10",title:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",description:"Weeks 8-10 - NVIDIA Isaac Sim, Isaac ROS, and Nav2 for perception and path planning in humanoid robotics",source:"@site/docs/week-8-10/week-8-10.md",sourceDirName:"week-8-10",slug:"/week-8-10/",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/week-8-10/",draft:!1,unlisted:!1,editUrl:"https://github.com/JaveriaNigar/New-Physical-AI-and-Humanoid-Robotics/docs/week-8-10/week-8-10.md",tags:[{label:"NVIDIA Isaac",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/nvidia-isaac"},{label:"Isaac Sim",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/isaac-sim"},{label:"Isaac ROS",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/isaac-ros"},{label:"Nav2",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/nav-2"},{label:"Path Planning",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/path-planning"},{label:"Humanoid Robotics",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/humanoid-robotics"},{label:"Perception",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/perception"}],version:"current",frontMatter:{title:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",description:"Weeks 8-10 - NVIDIA Isaac Sim, Isaac ROS, and Nav2 for perception and path planning in humanoid robotics",tags:["NVIDIA Isaac","Isaac Sim","Isaac ROS","Nav2","Path Planning","Humanoid Robotics","Perception"]}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Week 8: NVIDIA Isaac Sim Overview and Setup",id:"week-8-nvidia-isaac-sim-overview-and-setup",level:2},{value:"1. Introduction to NVIDIA Isaac Platform",id:"1-introduction-to-nvidia-isaac-platform",level:3},{value:"2. Prerequisites and System Requirements",id:"2-prerequisites-and-system-requirements",level:3},{value:"3. Installing NVIDIA Isaac Sim",id:"3-installing-nvidia-isaac-sim",level:3},{value:"Option A: Using Docker (Recommended)",id:"option-a-using-docker-recommended",level:4},{value:"Option B: Native Installation",id:"option-b-native-installation",level:4},{value:"4. Verifying Installation",id:"4-verifying-installation",level:3},{value:"5. Basic Isaac Sim Concepts",id:"5-basic-isaac-sim-concepts",level:3},{value:"5.1 Scene Graph and USD",id:"51-scene-graph-and-usd",level:4},{value:"5.2 Extensions",id:"52-extensions",level:4},{value:"5.3 Physics Simulation",id:"53-physics-simulation",level:4},{value:"6. Your First Isaac Sim Project",id:"6-your-first-isaac-sim-project",level:3},{value:"7. Exercises",id:"7-exercises",level:3},{value:"8. Summary",id:"8-summary",level:3},{value:"Week 9: Isaac ROS - Perception and Hardware Acceleration",id:"week-9-isaac-ros---perception-and-hardware-acceleration",level:2},{value:"1. Introduction to Isaac ROS",id:"1-introduction-to-isaac-ros",level:3},{value:"2. Core Isaac ROS Packages",id:"2-core-isaac-ros-packages",level:3},{value:"2.1 Isaac ROS AprilTag",id:"21-isaac-ros-apriltag",level:4},{value:"2.2 Isaac ROS Stereo DNN",id:"22-isaac-ros-stereo-dnn",level:4},{value:"2.3 Isaac ROS VSLAM",id:"23-isaac-ros-vslam",level:4},{value:"2.4 Isaac ROS Point Cloud",id:"24-isaac-ros-point-cloud",level:4},{value:"3. Perception Pipelines",id:"3-perception-pipelines",level:3},{value:"4. Hardware Acceleration Features",id:"4-hardware-acceleration-features",level:3},{value:"4.1 GPU Acceleration",id:"41-gpu-acceleration",level:4},{value:"4.2 CUDA Graphs",id:"42-cuda-graphs",level:4},{value:"4.3 Memory Management",id:"43-memory-management",level:4},{value:"5. Real-time Perception Example",id:"5-real-time-perception-example",level:3},{value:"6. Performance Optimization",id:"6-performance-optimization",level:3},{value:"6.1 Pipeline Optimization",id:"61-pipeline-optimization",level:4},{value:"6.2 GPU Utilization",id:"62-gpu-utilization",level:4},{value:"7. Exercises",id:"7-exercises-1",level:3},{value:"8. Summary",id:"8-summary-1",level:3},{value:"Week 10: Nav2 - Path Planning for Bipedal Humanoid Movement",id:"week-10-nav2---path-planning-for-bipedal-humanoid-movement",level:2},{value:"1. Introduction to Nav2",id:"1-introduction-to-nav2",level:3},{value:"2. Nav2 Architecture",id:"2-nav2-architecture",level:3},{value:"3. Nav2 for Humanoid Robots",id:"3-nav2-for-humanoid-robots",level:3},{value:"4. Nav2 Configuration for Humanoids",id:"4-nav2-configuration-for-humanoids",level:3},{value:"5. Humanoid-Specific Path Planning",id:"5-humanoid-specific-path-planning",level:3},{value:"5.1 Footstep Planning",id:"51-footstep-planning",level:4},{value:"6. Humanoid Navigation Launch File",id:"6-humanoid-navigation-launch-file",level:3},{value:"7. Integration with Isaac Sim",id:"7-integration-with-isaac-sim",level:3},{value:"8. Exercises",id:"8-exercises",level:3},{value:"9. Troubleshooting Common Issues",id:"9-troubleshooting-common-issues",level:3},{value:"9.1 Navigation Failures",id:"91-navigation-failures",level:4},{value:"9.2 Performance Issues",id:"92-performance-issues",level:4},{value:"10. Summary",id:"10-summary",level:3},{value:"Module Summary: The AI-Robot Brain (NVIDIA Isaac\u2122)",id:"module-summary-the-ai-robot-brain-nvidia-isaac",level:2},{value:"Quiz/Reflection Questions",id:"quizreflection-questions",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"module-3-the-ai-robot-brain-nvidia-isaac",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"In this module, we'll explore the NVIDIA Isaac Platform, which consists of Isaac Sim for simulation and Isaac ROS for perception and navigation capabilities. We'll learn how to leverage these tools to build intelligent humanoid robots that can understand and navigate their environment."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this module (Weeks 8-10), you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Install and set up the NVIDIA Isaac Sim environment"}),"\n",(0,a.jsx)(n.li,{children:"Work with Isaac ROS packages for perception and hardware acceleration"}),"\n",(0,a.jsx)(n.li,{children:"Implement navigation systems using Nav2 for bipedal humanoid movement"}),"\n",(0,a.jsx)(n.li,{children:"Create perception pipelines to enable robot awareness"}),"\n",(0,a.jsx)(n.li,{children:"Design path planning algorithms for humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Integrate perception and navigation for complete autonomous behavior"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"week-8-nvidia-isaac-sim-overview-and-setup",children:"Week 8: NVIDIA Isaac Sim Overview and Setup"}),"\n",(0,a.jsx)(n.p,{children:"NVIDIA Isaac Sim is a powerful robotics simulator built on NVIDIA Omniverse. It provides a highly realistic physics environment for developing, testing, and validating robotics applications before deploying to real hardware."}),"\n",(0,a.jsx)(n.h3,{id:"1-introduction-to-nvidia-isaac-platform",children:"1. Introduction to NVIDIA Isaac Platform"}),"\n",(0,a.jsx)(n.p,{children:"The NVIDIA Isaac Platform is a comprehensive robotics development platform that includes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Isaac Sim: High-fidelity simulation environment"}),"\n",(0,a.jsx)(n.li,{children:"Isaac ROS: ROS 2 packages for perception and navigation"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Navigation: Path planning and navigation solutions"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Manipulation: Tools for robot manipulation tasks"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Apps: Pre-built applications for common robotics tasks"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-prerequisites-and-system-requirements",children:"2. Prerequisites and System Requirements"}),"\n",(0,a.jsx)(n.p,{children:"Before installing NVIDIA Isaac Sim, ensure your system meets the following requirements:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check if you have a compatible NVIDIA GPU\r\nnvidia-smi\r\n\r\n# Verify your current ROS 2 installation\r\nros2 --version\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Hardware Requirements:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"NVIDIA GPU with CUDA Compute Capability 6.0 or higher (with real-time ray-tracing support strongly recommended)"}),"\n",(0,a.jsx)(n.li,{children:"At least 16GB system RAM (32GB recommended)"}),"\n",(0,a.jsx)(n.li,{children:"Ubuntu 20.04 LTS or 22.04 LTS"}),"\n",(0,a.jsx)(n.li,{children:"At least 20GB of available disk space"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Software Requirements:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Ubuntu 20.04 LTS or 22.04 LTS"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,a.jsx)(n.li,{children:"Docker and NVIDIA Container Toolkit (recommended)"}),"\n",(0,a.jsx)(n.li,{children:"NVIDIA GPU drivers (version 520 or later)"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-installing-nvidia-isaac-sim",children:"3. Installing NVIDIA Isaac Sim"}),"\n",(0,a.jsx)(n.h4,{id:"option-a-using-docker-recommended",children:"Option A: Using Docker (Recommended)"}),"\n",(0,a.jsx)(n.p,{children:"Docker provides the most reliable way to run Isaac Sim:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Pull the latest Isaac Sim container\r\ndocker pull nvcr.io/nvidia/isaac-sim:latest\r\n\r\n# Run Isaac Sim container (replace with your actual paths)\r\ndocker run --gpus all -it --rm \\\r\n  --network=host \\\r\n  --env NVIDIA_VISIBLE_DEVICES=all \\\r\n  --env NVIDIA_DRIVER_CAPABILITIES=all \\\r\n  --env DISPLAY \\\r\n  --env PYTHONPATH=/isaac_sim/python \\\r\n  --volume $HOME/.Xauthority:/root/.Xauthority \\\r\n  --volume $HOME/isaac-sim/projects:/projects \\\r\n  --volume $HOME/isaac-sim/cache/kit:/isaac-sim/kit/cache \\\r\n  --volume $HOME/isaac-sim/cache/ov:/root/.cache/ov \\\r\n  --volume $HOME/isaac-sim/logs:/root/.nvidia-omniverse/logs \\\r\n  --volume $HOME/isaac-sim/config:/root/.nvidia-omniverse/config \\\r\n  --volume $HOME/isaac-sim/data:/ov/data \\\r\n  --privileged \\\r\n  --pid=host \\\r\n  nvcr.io/nvidia/isaac-sim:latest\n"})}),"\n",(0,a.jsx)(n.h4,{id:"option-b-native-installation",children:"Option B: Native Installation"}),"\n",(0,a.jsx)(n.p,{children:"For native installation, follow these steps:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# 1. Clone the Isaac Sim repository\r\ngit clone https://github.com/NVIDIA-Omniverse/isaac-sim.git\r\ncd isaac-sim\r\n\r\n# 2. Install prerequisites\r\nsudo apt update\r\nsudo apt install -y python3 python3-pip python3-venv build-essential\r\n\r\n# 3. Create a virtual environment\r\npython3 -m venv ~/isaac-sim-env\r\nsource ~/isaac-sim-env/bin/activate\r\n\r\n# 4. Install Isaac Sim\r\npip install -e .\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-verifying-installation",children:"4. Verifying Installation"}),"\n",(0,a.jsx)(n.p,{children:"After installation, verify that Isaac Sim is working correctly:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# If using native installation\r\nsource ~/isaac-sim-env/bin/activate\r\npython -c \"import omni; print('Isaac Sim Python API is available!')\"\r\n\r\n# If using Docker, run the container and execute:\r\n# python -c \"import omni; print('Isaac Sim Python API is available!')\"\n"})}),"\n",(0,a.jsx)(n.h3,{id:"5-basic-isaac-sim-concepts",children:"5. Basic Isaac Sim Concepts"}),"\n",(0,a.jsx)(n.h4,{id:"51-scene-graph-and-usd",children:"5.1 Scene Graph and USD"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim uses Universal Scene Description (USD) as its core data model:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"USD is a 3D scene representation that enables complex scene composition"}),"\n",(0,a.jsx)(n.li,{children:"It allows for layering, referencing, and instancing of complex scenes"}),"\n",(0,a.jsx)(n.li,{children:"USD supports multiple backends (Physics, Rendering, etc.)"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"52-extensions",children:"5.2 Extensions"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim provides various extensions for robotics development:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"omni.isaac.ros_bridge"}),": Connects Isaac Sim with ROS 2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"omni.isaac.range_sensor"}),": Provides range sensor support (lidar, depth camera)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"omni.isaac.motion_generation"}),": Provides motion planning capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"omni.isaac.sensor"}),": Provides sensor support for cameras and IMUs"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"53-physics-simulation",children:"5.3 Physics Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim supports two physics engines:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"PhysX: NVIDIA's physics engine optimized for robotics"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Sim also provides accurate collision detection and response"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"6-your-first-isaac-sim-project",children:"6. Your First Isaac Sim Project"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a simple scene in Isaac Sim:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# First_isaac_sim.py\r\nimport omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.prims import RigidPrim, XFormPrim\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\n\r\n# Initialize the world\r\nmy_world = World(stage_units_in_meters=1.0)\r\n\r\n# Add a ground plane and a simple cube\r\nmy_world.scene.add_default_ground_plane()\r\ncube = my_world.scene.add(XFormPrim(prim_path="/World/cube", name="my_cube", position=[0, 0, 1]))\r\ncube.set_world_pose(position=[0, 0, 1], orientation=[0, 0, 0, 1])\r\n\r\n# Play the simulation\r\nmy_world.reset()\r\nfor i in range(1000):\r\n    my_world.step(render=True)\r\n\r\nmy_world.clear()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"7-exercises",children:"7. Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Basic Scene Creation"}),": Create a scene with multiple objects of different shapes (cube, sphere, cylinder) and colors."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple Physics Simulation"}),": Create a scene with a static ground and a stack of falling cubes. Observe how they interact with the physics engine."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera Integration"}),": Add a camera to your scene and capture images from different viewpoints."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"8-summary",children:"8. Summary"}),"\n",(0,a.jsx)(n.p,{children:"This week, we covered the basics of NVIDIA Isaac Sim, including installation, core concepts, and creating your first simulation. Isaac Sim provides the foundation for physics-accurate simulations that bridge the gap between development and real-world deployment."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"week-9-isaac-ros---perception-and-hardware-acceleration",children:"Week 9: Isaac ROS - Perception and Hardware Acceleration"}),"\n",(0,a.jsx)(n.p,{children:"The NVIDIA Isaac ROS packages bring hardware acceleration to traditional robotics perception tasks. These packages leverage the GPU for real-time perception tasks such as depth estimation, SLAM, and computer vision."}),"\n",(0,a.jsx)(n.h3,{id:"1-introduction-to-isaac-ros",children:"1. Introduction to Isaac ROS"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS is a collection of hardware-accelerated ROS 2 packages that perform perception, navigation, and manipulation tasks. These packages are optimized for NVIDIA GPUs and run much faster than their CPU equivalents."}),"\n",(0,a.jsx)(n.h3,{id:"2-core-isaac-ros-packages",children:"2. Core Isaac ROS Packages"}),"\n",(0,a.jsx)(n.h4,{id:"21-isaac-ros-apriltag",children:"2.1 Isaac ROS AprilTag"}),"\n",(0,a.jsx)(n.p,{children:"AprilTag detection optimized for NVIDIA GPUs:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Run Isaac ROS AprilTag detection\r\nros2 launch isaac_ros_apriltag_apriltag.launch.py\n"})}),"\n",(0,a.jsx)(n.h4,{id:"22-isaac-ros-stereo-dnn",children:"2.2 Isaac ROS Stereo DNN"}),"\n",(0,a.jsx)(n.p,{children:"Real-time stereo depth estimation using deep neural networks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Run Isaac ROS Stereo DNN\r\nros2 launch isaac_ros_stereo_dnn_stereo_dnn.launch.py\n"})}),"\n",(0,a.jsx)(n.h4,{id:"23-isaac-ros-vslam",children:"2.3 Isaac ROS VSLAM"}),"\n",(0,a.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Run Isaac ROS VSLAM\r\nros2 launch isaac_ros_visual_slam_visual_slam.launch.py\n"})}),"\n",(0,a.jsx)(n.h4,{id:"24-isaac-ros-point-cloud",children:"2.4 Isaac ROS Point Cloud"}),"\n",(0,a.jsx)(n.p,{children:"Converting depth images to point clouds:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Run Isaac ROS Point Cloud\r\nros2 launch isaac_ros_point_cloud_point_cloud.launch.py\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-perception-pipelines",children:"3. Perception Pipelines"}),"\n",(0,a.jsx)(n.p,{children:"Let's create an example perception pipeline with Isaac ROS:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# perception_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass PerceptionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__('perception_pipeline')\r\n        \r\n        # Create a subscription to the image topic\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_rect_color',\r\n            self.image_callback,\r\n            10)\r\n        \r\n        # Create a publisher for processed images\r\n        self.publisher = self.create_publisher(\r\n            Image,\r\n            '/camera/processed_image',\r\n            10)\r\n        \r\n        self.bridge = CvBridge()\r\n        self.get_logger().info('Perception Pipeline Node Started')\r\n        \r\n    def image_callback(self, msg):\r\n        # Convert ROS Image message to OpenCV image\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n        \r\n        # Process the image (example: edge detection)\r\n        processed_image = cv2.Canny(cv_image, 100, 200)\r\n        \r\n        # Convert back to ROS Image message\r\n        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='mono8')\r\n        processed_msg.header = msg.header\r\n        \r\n        # Publish the processed image\r\n        self.publisher.publish(processed_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    perception_pipeline = PerceptionPipeline()\r\n    rclpy.spin(perception_pipeline)\r\n    perception_pipeline.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-hardware-acceleration-features",children:"4. Hardware Acceleration Features"}),"\n",(0,a.jsx)(n.h4,{id:"41-gpu-acceleration",children:"4.1 GPU Acceleration"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS packages leverage NVIDIA GPUs for accelerated processing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Tensor Cores for deep learning inference"}),"\n",(0,a.jsx)(n.li,{children:"CUDA cores for general compute tasks"}),"\n",(0,a.jsx)(n.li,{children:"Dedicated video processing units for video encoding/decoding"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"42-cuda-graphs",children:"4.2 CUDA Graphs"}),"\n",(0,a.jsx)(n.p,{children:"For real-time applications, CUDA Graphs can be used to optimize execution:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Enable CUDA graphs for Isaac ROS packages (where supported)\r\nexport ISAAC_ROS_CUDAGRAPH_MODE=1\n"})}),"\n",(0,a.jsx)(n.h4,{id:"43-memory-management",children:"4.3 Memory Management"}),"\n",(0,a.jsx)(n.p,{children:"Optimize memory usage in perception pipelines:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use zero-copy memory transfers where possible"}),"\n",(0,a.jsx)(n.li,{children:"Allocate memory pools for repeated operations"}),"\n",(0,a.jsx)(n.li,{children:"Implement efficient memory management for real-time requirements"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"5-real-time-perception-example",children:"5. Real-time Perception Example"}),"\n",(0,a.jsx)(n.p,{children:"Here's an example of a complete real-time perception system using Isaac ROS:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# real_time_perception.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import Point\r\nfrom vision_msgs.msg import Detection2DArray\r\nimport numpy as np\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nfrom std_msgs.msg import String\r\n\r\nclass RealTimePerception(Node):\r\n    def __init__(self):\r\n        super().__init__('real_time_perception')\r\n        \r\n        # Subscribe to camera topics\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_rect_color',\r\n            self.image_callback,\r\n            10)\r\n        \r\n        self.info_subscription = self.create_subscription(\r\n            CameraInfo,\r\n            '/camera/rgb/camera_info',\r\n            self.info_callback,\r\n            10)\r\n            \r\n        # Publishers for processed data\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            '/detections',\r\n            10)\r\n            \r\n        self.debug_publisher = self.create_publisher(\r\n            Image,\r\n            '/debug_image',\r\n            10)\r\n        \r\n        self.bridge = CvBridge()\r\n        self.camera_info = None\r\n        \r\n        self.get_logger().info('Real Time Perception Node Started')\r\n        \r\n    def info_callback(self, msg):\r\n        self.camera_info = msg\r\n        \r\n    def image_callback(self, msg):\r\n        # Convert ROS Image message to OpenCV image\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n        \r\n        # Process image to detect objects (simple example)\r\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\r\n        \r\n        # Detect circles using HoughCircles (for example)\r\n        circles = cv2.HoughCircles(\r\n            gray, \r\n            cv2.HOUGH_GRADIENT, \r\n            1, \r\n            20, \r\n            param1=50, \r\n            param2=30, \r\n            minRadius=10, \r\n            maxRadius=100\r\n        )\r\n        \r\n        # Draw detected circles on debug image\r\n        debug_image = cv_image.copy()\r\n        if circles is not None:\r\n            circles = np.round(circles[0, :]).astype(\"int\")\r\n            for (x, y, r) in circles:\r\n                cv2.circle(debug_image, (x, y), r, (0, 255, 0), 4)\r\n        \r\n        # Create and publish detections\r\n        detections = Detection2DArray()\r\n        if circles is not None:\r\n            for (x, y, r) in circles:\r\n                detection = Detection2D()\r\n                detection.bbox.center.x = x\r\n                detection.bbox.center.y = y\r\n                detection.bbox.size_x = 2 * r\r\n                detection.bbox.size_y = 2 * r\r\n                detections.detections.append(detection)\r\n        \r\n        detections.header = msg.header\r\n        self.detection_publisher.publish(detections)\r\n        \r\n        # Publish debug image\r\n        debug_msg = self.bridge.cv2_to_imgmsg(debug_image, encoding='bgr8')\r\n        debug_msg.header = msg.header\r\n        self.debug_publisher.publish(debug_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    real_time_perception = RealTimePerception()\r\n    rclpy.spin(real_time_perception)\r\n    real_time_perception.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"6-performance-optimization",children:"6. Performance Optimization"}),"\n",(0,a.jsx)(n.h4,{id:"61-pipeline-optimization",children:"6.1 Pipeline Optimization"}),"\n",(0,a.jsx)(n.p,{children:"To achieve real-time performance:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Minimize data copying"}),": Use zero-copy mechanisms where possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Threading"}),": Use separate threads for different stages of the pipeline"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch processing"}),": Process multiple frames together when possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory management"}),": Reuse buffers and avoid memory allocation during processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"62-gpu-utilization",children:"6.2 GPU Utilization"}),"\n",(0,a.jsx)(n.p,{children:"Monitor GPU utilization during perception tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Monitor GPU usage\r\nnvidia-smi -l 1\r\n\r\n# Check GPU memory usage specifically for Isaac ROS\r\nnvidia-ml-py3 # Python library for GPU monitoring\n"})}),"\n",(0,a.jsx)(n.h3,{id:"7-exercises-1",children:"7. Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object Detection Pipeline"}),": Implement a perception pipeline that detects and tracks objects using Isaac ROS packages."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Estimation"}),": Use Isaac ROS Stereo DNN to generate depth maps from stereo images."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SLAM Integration"}),": Set up Isaac ROS VSLAM and visualize the resulting map."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance Analysis"}),": Measure the performance of different perception tasks and identify bottlenecks."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"8-summary-1",children:"8. Summary"}),"\n",(0,a.jsx)(n.p,{children:"This week, we explored Isaac ROS packages for perception tasks and learned how to leverage GPU acceleration for real-time robotics applications. These tools are essential for building intelligent robots that can perceive and understand their environment."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"week-10-nav2---path-planning-for-bipedal-humanoid-movement",children:"Week 10: Nav2 - Path Planning for Bipedal Humanoid Movement"}),"\n",(0,a.jsx)(n.p,{children:"Navigation is a critical component for mobile robots, and the Navigation2 (Nav2) stack provides a comprehensive framework for path planning and execution. In this week, we'll focus specifically on adapting Nav2 for bipedal humanoid robots."}),"\n",(0,a.jsx)(n.h3,{id:"1-introduction-to-nav2",children:"1. Introduction to Nav2"}),"\n",(0,a.jsx)(n.p,{children:"Nav2 is the next-generation navigation stack for ROS 2, designed to replace the ROS 1 navigation stack. It features a behavior tree-based architecture that allows for more complex navigation behaviors and better fault tolerance."}),"\n",(0,a.jsx)(n.h3,{id:"2-nav2-architecture",children:"2. Nav2 Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Nav2 consists of several key components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Navigation Server"}),": Coordinates all navigation tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Planner Server"}),": Generates global path plans"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Controller Server"}),": Generates local velocity commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Recovery Server"}),": Handles failures with recovery behaviors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lifecycle Manager"}),": Manages the lifecycle of navigation components"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-nav2-for-humanoid-robots",children:"3. Nav2 for Humanoid Robots"}),"\n",(0,a.jsx)(n.p,{children:"Unlike wheeled robots, bipedal humanoid robots have unique navigation challenges:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Balance"}),": The robot must maintain balance while navigating"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ZMP (Zero-Moment Point)"}),": Consider balance point during movement"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Step Planning"}),": Instead of continuous motion, humanoid robots move in discrete steps"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Terrain Adaptability"}),": Ability to navigate uneven terrain that may require stepping"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-nav2-configuration-for-humanoids",children:"4. Nav2 Configuration for Humanoids"}),"\n",(0,a.jsx)(n.p,{children:"Here's a sample Nav2 configuration for a humanoid robot:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# nav2_params_humanoid.yaml\r\namcl:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n    alpha1: 0.2\r\n    alpha2: 0.2\r\n    alpha3: 0.2\r\n    alpha4: 0.2\r\n    alpha5: 0.2\r\n    base_frame_id: "base_footprint"\r\n    beam_skip_distance: 0.5\r\n    beam_skip_error_threshold: 0.9\r\n    beam_skip_threshold: 0.3\r\n    do_beamskip: false\r\n    global_frame_id: "map"\r\n    lambda_short: 0.1\r\n    laser_likelihood_max_dist: 2.0\r\n    laser_max_range: 100.0\r\n    laser_min_range: -1.0\r\n    laser_model_type: "likelihood_field"\r\n    max_beams: 60\r\n    max_particles: 2000\r\n    min_particles: 500\r\n    odom_frame_id: "odom"\r\n    pf_err: 0.05\r\n    pf_z: 0.99\r\n    recovery_alpha_fast: 0.0\r\n    recovery_alpha_slow: 0.0\r\n    resample_interval: 1\r\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\r\n    save_pose_delay: 0.5\r\n    set_initial_pose: false\r\n    tf_broadcast: true\r\n    transform_tolerance: 1.0\r\n    update_min_a: 0.2\r\n    update_min_d: 0.25\r\n    z_hit: 0.5\r\n    z_max: 0.05\r\n    z_rand: 0.5\r\n    z_short: 0.05\r\n\r\namcl_map_client:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n\r\namcl_rclcpp_node:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n\r\nbt_navigator:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n    global_frame: map\r\n    robot_base_frame: base_footprint\r\n    odom_topic: /odom\r\n    bt_loop_duration: 10\r\n    default_server_timeout: 20\r\n    # Specify the path where the behavior tree XML files are located\r\n    plugin_lib_names:\r\n    - nav2_compute_path_to_pose_action_bt_node\r\n    - nav2_compute_path_through_poses_action_bt_node\r\n    - nav2_follow_path_action_bt_node\r\n    - nav2_spin_action_bt_node\r\n    - nav2_wait_action_bt_node\r\n    - nav2_assisted_teleop_action_bt_node\r\n    - nav2_back_up_action_bt_node\r\n    - nav2_drive_on_heading_bt_node\r\n    - nav2_clear_costmap_service_bt_node\r\n    - nav2_is_stuck_condition_bt_node\r\n    - nav2_goal_reached_condition_bt_node\r\n    - nav2_goal_updated_condition_bt_node\r\n    - nav2_initial_pose_received_condition_bt_node\r\n    - nav2_reinitialize_global_localization_service_bt_node\r\n    - nav2_rate_controller_bt_node\r\n    - nav2_distance_controller_bt_node\r\n    - nav2_speed_controller_bt_node\r\n    - nav2_truncate_path_action_bt_node\r\n    - nav2_truncate_path_local_action_bt_node\r\n    - nav2_goal_updater_node_bt_node\r\n    - nav2_recovery_node_bt_node\r\n    - nav2_pipeline_sequence_bt_node\r\n    - nav2_round_robin_node_bt_node\r\n    - nav2_transform_available_condition_bt_node\r\n    - nav2_time_expired_condition_bt_node\r\n    - nav2_path_expiring_timer_condition\r\n    - nav2_distance_traveled_condition_bt_node\r\n    - nav2_single_trigger_bt_node\r\n    - nav2_is_battery_low_condition_bt_node\r\n    - nav2_navigate_through_poses_action_bt_node\r\n    - nav2_navigate_to_pose_action_bt_node\r\n    - nav2_remove_passed_goals_action_bt_node\r\n    - nav2_planner_selector_bt_node\r\n    - nav2_controller_selector_bt_node\r\n    - nav2_goal_checker_selector_bt_node\r\n    - nav2_controller_cancel_bt_node\r\n    - nav2_path_longer_on_approach_bt_node\r\n    - nav2_wait_cancel_bt_node\r\n    - nav2_spin_cancel_bt_node\r\n    - nav2_back_up_cancel_bt_node\r\n    - nav2_assisted_teleop_cancel_bt_node\r\n    - nav2_drive_on_heading_cancel_bt_node\r\n\r\nbt_navigator_rclcpp_node:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n\r\nvelocity_smoother:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n    smoothing_frequency: 20.0\r\n    scale_velocities: false\r\n    feedback: "OPEN_LOOP"\r\n    velocity_scaling_tolerance: 1.0\r\n    velocity_threshold: 0.0\r\n    stateful: true\r\n    acceleration_limits: [2.5, 2.5, 3.2]\r\n    acceleration_limits_dimensions: [0, 1, 2]\r\n    deceleration_limits: [2.5, 2.5, 3.2]\r\n    deceleration_limits_dimensions: [0, 1, 2]\r\n    velocity_scaling: 1.0\r\n\r\ncontroller_server:\r\n  ros__parameters:\r\n    use_sim_time: True\r\n    controller_frequency: 20.0\r\n    min_x_velocity_threshold: 0.001\r\n    min_y_velocity_threshold: 0.5\r\n    min_theta_velocity_threshold: 0.001\r\n    # Use the humanoid-specific controller\r\n    progress_checker_plugin: "progress_checker"\r\n    goal_checker_plugin: "goal_checker"\r\n    controller_plugins: ["FollowPathHumanoid"]\r\n\r\n    # Humanoid controller\r\n    FollowPathHumanoid:\r\n      plugin: "nav2_mppi_controller::Controller"\r\n      time_steps: 50\r\n      control_frequency: 20.0\r\n      dt: 0.05\r\n      motion_samples: 100\r\n      aux_distance: 1.0\r\n      aux_speed: 0.4\r\n      reference_speed: 0.3\r\n      max_linear_speed: 0.5\r\n      max_angular_speed: 0.3\r\n      max_linear_accel: 0.3\r\n      max_angular_accel: 0.3\r\n      xy_goal_tolerance: 0.25\r\n      yaw_goal_tolerance: 0.25\r\n      debug_weights: false\r\n      transform_tolerance: 0.3\r\n      use_interpolation: true\r\n      model_type: "nav2_mppi_controller::HumanoidModel"\r\n      model_options:\r\n        wheelbase: 0.3\r\n        max_steer: 0.5\r\n\r\nlocal_costmap:\r\n  local_costmap:\r\n    ros__parameters:\r\n      update_frequency: 5.0\r\n      publish_frequency: 2.0\r\n      global_frame: odom\r\n      robot_base_frame: base_footprint\r\n      use_sim_time: True\r\n      rolling_window: true\r\n      width: 3\r\n      height: 3\r\n      resolution: 0.05\r\n      robot_radius: 0.3  # Humanoid robot radius\r\n      plugins: ["voxel_layer", "inflation_layer"]\r\n      inflation_layer:\r\n        plugin: "nav2_costmap_2d::InflationLayer"\r\n        cost_scaling_factor: 3.0\r\n        inflation_radius: 0.55\r\n      obstacle_layer:\r\n        plugin: "nav2_costmap_2d::VoxelLayer"\r\n        enabled: True\r\n        voxel_size: 0.05\r\n        max_voxels: 10000\r\n        mark_threshold: 0\r\n        observation_sources: scan\r\n        scan:\r\n          topic: /scan\r\n          max_obstacle_height: 2.0\r\n          clearing: True\r\n          marking: True\r\n          data_type: "LaserScan"\r\n          raytrace_max_range: 3.0\r\n          raytrace_min_range: 0.0\r\n          obstacle_max_range: 2.5\r\n          obstacle_min_range: 0.0\r\n      always_send_full_costmap: True\r\n  local_costmap_client:\r\n    ros__parameters:\r\n      use_sim_time: True\r\n  local_costmap_rclcpp_node:\r\n    ros__parameters:\r\n      use_sim_time: True\r\n\r\nglobal_costmap:\r\n  global_costmap:\r\n    ros__parameters:\r\n      update_frequency: 1.0\r\n      publish_frequency: 1.0\r\n      global_frame: map\r\n      robot_base_frame: base_footprint\r\n      use_sim_time: True\r\n      robot_radius: 0.3  # Humanoid robot radius\r\n      resolution: 0.05\r\n      plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\r\n      obstacle_layer:\r\n        plugin: "nav2_costmap_2d::VoxelLayer"\r\n        enabled: True\r\n        voxel_size: 0.05\r\n        max_voxels: 10000\r\n        mark_threshold: 0\r\n        observation_sources: scan\r\n        scan:\r\n          topic: /scan\r\n          max_obstacle_height: 2.0\r\n          clearing: True\r\n          marking: True\r\n          data_type: "LaserScan"\r\n          raytrace_max_range: 3.0\r\n          raytrace_min_range: 0.0\r\n          obstacle_max_range: 2.5\r\n          obstacle_min_range: 0.0\r\n      static_layer:\r\n        plugin: "nav2_costmap_2d::StaticLayer"\r\n        map_subscribe_transient_local: True\r\n      inflation_layer:\r\n        plugin: "nav2_costmap_2d::InflationLayer"\r\n        cost_scaling_factor: 3.0\r\n        inflation_radius: 0.55\r\n      always_send_full_costmap: True\r\n  global_costmap_client:\r\n    ros__parameters:\r\n      use_sim_time: True\r\n  global_costmap_rclcpp_node:\r\n    ros__parameters:\r\n      use_sim_time: True\r\n\r\nplanner_server:\r\n  ros__parameters:\r\n    expected_planner_frequency: 20.0\r\n    use_sim_time: True\r\n    planner_plugins: ["GridBased"]\r\n    GridBased:\r\n      plugin: "nav2_navfn_planner::NavfnPlanner"\r\n      tolerance: 0.5\r\n      use_astar: false\r\n      allow_unknown: true\r\n\r\nsmoother_server:\r\n  ros__parameters:\r\n    smoother_frequency: 20.0\r\n    max_angular_velocity: 0.3\r\n    max_linear_velocity: 0.5\r\n    max_angular_acceleration: 0.3\r\n    max_linear_acceleration: 0.3\r\n    use_interpolation: true\r\n    smoother_plugins: ["simple_smoother"]\r\n    simple_smoother:\r\n      plugin: "nav2_smoother::SimpleSmoother"\r\n      tolerance: 1.0e-10\r\n      max_its: 1000\r\n      do_refinement: true\r\n\r\nbehavior_server:\r\n  ros__parameters:\r\n    costmap_topic: local_costmap/costmap_raw\r\n    footprint_topic: local_costmap/published_footprint\r\n    cycle_frequency: 10.0\r\n    behavior_plugins: ["spin", "backup", "drive_on_heading", "assisted_teleop"]\r\n    spin:\r\n      plugin: "nav2_behaviors/Spin"\r\n      spin_dist: 1.57\r\n    backup:\r\n      plugin: "nav2_behaviors/BackUp"\r\n      backup_dist: 0.15\r\n      backup_speed: 0.025\r\n    drive_on_heading:\r\n      plugin: "nav2_behaviors/DriveOnHeading"\r\n      drive_on_heading_max_linear_speed: 0.3\r\n      drive_on_heading_max_angular_speed: 0.3\r\n    assisted_teleop:\r\n      plugin: "nav2_behaviors/AssistedTeleop"\r\n      assisted_teleop_max_linear_speed: 0.3\r\n      assisted_teleop_max_angular_speed: 0.3\r\n      assisted_teleop_min_obstacle_dist: 0.1\r\n    local_frame: odom\r\n    global_frame: map\r\n    robot_base_frame: base_footprint\r\n    transform_tolerance: 0.3\r\n    use_sim_time: True\r\n\r\nrobot_state_publisher:\r\n  ros__parameters:\r\n    use_sim_time: True\n'})}),"\n",(0,a.jsx)(n.h3,{id:"5-humanoid-specific-path-planning",children:"5. Humanoid-Specific Path Planning"}),"\n",(0,a.jsx)(n.p,{children:"For bipedal robots, navigation needs to consider:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Step Sequencing"}),": Generating a sequence of footstep positions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Balance Maintenance"}),": Ensuring the robot's center of mass remains stable"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Terrain Analysis"}),": Detecting suitable footholds on uneven terrain"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-Contact Planning"}),": Planning for multiple contact points during movement"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"51-footstep-planning",children:"5.1 Footstep Planning"}),"\n",(0,a.jsx)(n.p,{children:"Here's an example of footstep planning for humanoid navigation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# humanoid_footstep_planner.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom nav_msgs.msg import Path\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom visualization_msgs.msg import MarkerArray, Marker\r\nimport numpy as np\r\n\r\nclass HumanoidFootstepPlanner(Node):\r\n    def __init__(self):\r\n        super().__init__(\'humanoid_footstep_planner\')\r\n        \r\n        self.path_sub = self.create_subscription(\r\n            Path,\r\n            \'global_plan\',\r\n            self.path_callback,\r\n            10)\r\n        \r\n        self.footstep_pub = self.create_publisher(\r\n            MarkerArray,\r\n            \'footsteps\',\r\n            10)\r\n        \r\n        # Parameters\r\n        self.step_length = 0.3  # distance between consecutive steps\r\n        self.step_width = 0.2   # distance between left and right feet\r\n        self.step_height = 0.1  # height of step motion (for stepping over obstacles)\r\n        \r\n        self.get_logger().info(\'Humanoid Footstep Planner Initialized\')\r\n    \r\n    def path_callback(self, msg):\r\n        """Convert global plan to footstep sequence"""\r\n        if len(msg.poses) < 2:\r\n            return\r\n        \r\n        # Generate footsteps based on the global path\r\n        footsteps = self.generate_footsteps(msg.poses)\r\n        \r\n        # Publish footsteps as visualization markers\r\n        self.publish_footsteps(footsteps)\r\n    \r\n    def generate_footsteps(self, poses):\r\n        """Generate left and right footsteps alternating along the path"""\r\n        footsteps = []\r\n        \r\n        # Start with left foot (arbitrary starting side)\r\n        left_foot = True\r\n        \r\n        # Iterate through path poses and generate footsteps\r\n        for i in range(len(poses) - 1):\r\n            # Calculate direction vector\r\n            dx = poses[i+1].pose.position.x - poses[i].pose.position.x\r\n            dy = poses[i+1].pose.position.y - poses[i].pose.position.y\r\n            dist = np.sqrt(dx*dx + dy*dy)\r\n            \r\n            # If the distance is significant, add a step\r\n            if dist > self.step_length / 2:\r\n                footstep = Pose()\r\n                footstep.position = poses[i].pose.position\r\n                \r\n                # Alternate between left and right foot\r\n                if left_foot:\r\n                    # Offset left foot to the left (positive Y in robot frame)\r\n                    footstep.position.y += self.step_width / 2\r\n                else:\r\n                    # Offset right foot to the right (negative Y in robot frame)\r\n                    footstep.position.y -= self.step_width / 2\r\n                \r\n                footsteps.append({\r\n                    \'pose\': footstep,\r\n                    \'left\': left_foot\r\n                })\r\n                \r\n                left_foot = not left_foot  # Alternate for next step\r\n        \r\n        return footsteps\r\n    \r\n    def publish_footsteps(self, footsteps):\r\n        """Publish footsteps as visualization markers"""\r\n        marker_array = MarkerArray()\r\n        \r\n        for i, footstep in enumerate(footsteps):\r\n            marker = Marker()\r\n            marker.header.frame_id = "map"\r\n            marker.header.stamp = self.get_clock().now().to_msg()\r\n            marker.ns = "footsteps"\r\n            marker.id = i\r\n            marker.type = Marker.CYLINDER\r\n            marker.action = Marker.ADD\r\n            \r\n            marker.pose = footstep[\'pose\']\r\n            marker.pose.position.z = 0.02  # Slightly above ground\r\n            \r\n            # Different colors for left and right feet\r\n            if footstep[\'left\']:\r\n                marker.color.r = 1.0  # Red for left foot\r\n                marker.color.g = 0.0\r\n                marker.color.b = 0.0\r\n            else:\r\n                marker.color.r = 0.0  # Blue for right foot\r\n                marker.color.g = 0.0\r\n                marker.color.b = 1.0\r\n            marker.color.a = 1.0\r\n            \r\n            marker.scale.x = 0.1  # Foot radius\r\n            marker.scale.y = 0.1  # Foot radius\r\n            marker.scale.z = 0.01 # Thickness\r\n            \r\n            marker_array.markers.append(marker)\r\n        \r\n        self.footstep_pub.publish(marker_array)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    planner = HumanoidFootstepPlanner()\r\n    rclpy.spin(planner)\r\n    planner.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"6-humanoid-navigation-launch-file",children:"6. Humanoid Navigation Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a launch file to bring up navigation for a humanoid robot:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- humanoid_navigation.launch.py --\x3e\r\nimport os\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, SetEnvironmentVariable\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\nfrom nav2_common.launch import RewrittenYaml\r\n\r\n\r\ndef generate_launch_description():\r\n    # Launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time')\r\n    autostart = LaunchConfiguration('autostart')\r\n    params_file = LaunchConfiguration('params_file')\r\n\r\n    # Declare launch arguments\r\n    declare_use_sim_time_arg = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='True',\r\n        description='Use simulation (Gazebo) clock if true')\r\n\r\n    declare_autostart_arg = DeclareLaunchArgument(\r\n        'autostart', \r\n        default_value='True',\r\n        description='Automatically startup the nav2 stack')\r\n\r\n    declare_params_file_arg = DeclareLaunchArgument(\r\n        'params_file',\r\n        default_value=os.path.join(\r\n            get_package_share_directory('my_humanoid_bringup'),\r\n            'config',\r\n            'nav2_params_humanoid.yaml'),\r\n        description='Full path to the ROS2 parameters file to use for all launched nodes')\r\n\r\n    # Create our own temporary YAML files that include substitutions\r\n    param_substitutions = {\r\n        'use_sim_time': use_sim_time,\r\n        'autostart': autostart}\r\n\r\n    configured_params = RewrittenYaml(\r\n        source_file=params_file,\r\n        root_key='navigation',\r\n        param_rewrites=param_substitutions,\r\n        convert_types=True)\r\n\r\n    # Navigation Server Node\r\n    nav2_node = Node(\r\n        package='nav2_navigation',\r\n        executable='navigation_server',\r\n        name='navigation_server',\r\n        output='screen',\r\n        parameters=[configured_params],\r\n        remappings=[('/cmd_vel', '/diffbot_base_controller/cmd_vel_unstamped')])\r\n\r\n    # Lifecycle Manager Node\r\n    lifecycle_manager = Node(\r\n        package='nav2_lifecycle_manager',\r\n        executable='lifecycle_manager',\r\n        name='lifecycle_manager_navigation',\r\n        output='screen',\r\n        parameters=[{'use_sim_time': use_sim_time},\r\n                    {'autostart': autostart},\r\n                    {'node_names': ['navigation_server',\r\n                                    'local_costmap_client',\r\n                                    'global_costmap_client',\r\n                                    'bt_navigator',\r\n                                    'controller_server',\r\n                                    'smoother_server',\r\n                                    'planner_server',\r\n                                    'behavior_server']}])\r\n\r\n    ld = LaunchDescription()\r\n\r\n    # Add the actions\r\n    ld.add_action(declare_use_sim_time_arg)\r\n    ld.add_action(declare_autostart_arg)\r\n    ld.add_action(declare_params_file_arg)\r\n\r\n    ld.add_action(nav2_node)\r\n    ld.add_action(lifecycle_manager)\r\n\r\n    return ld\n"})}),"\n",(0,a.jsx)(n.h3,{id:"7-integration-with-isaac-sim",children:"7. Integration with Isaac Sim"}),"\n",(0,a.jsx)(n.p,{children:"To integrate Nav2 with Isaac Sim for simulation:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Launch Isaac Sim"})," with your humanoid robot model"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Run the ROS bridge"})," to connect Isaac Sim with ROS 2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Launch Nav2"})," with the humanoid-specific configuration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Send navigation goals"})," to test the system"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"8-exercises",children:"8. Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Basic Navigation"}),": Set up Nav2 with a simple humanoid robot model in Isaac Sim and navigate to different goals."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Custom Path Planner"}),": Implement a custom path planner plugin for humanoid-specific navigation requirements."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Step Planning"}),": Develop a footstep planner that generates stable footsteps for navigating uneven terrain."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Balance Integration"}),": Integrate balance control with navigation to maintain stability during movement."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"9-troubleshooting-common-issues",children:"9. Troubleshooting Common Issues"}),"\n",(0,a.jsx)(n.h4,{id:"91-navigation-failures",children:"9.1 Navigation Failures"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Check costmaps"}),": Ensure local and global costmaps are updating properly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Verify transforms"}),": Ensure all tf transforms are broadcasting correctly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Tune parameters"}),": Adjust navigation parameters based on your robot's capabilities"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"92-performance-issues",children:"9.2 Performance Issues"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reduce frequency"}),": Lower the update frequency of navigation components if needed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimize path planning"}),": Use more efficient path planning algorithms for real-time performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simplify costmaps"}),": Reduce the resolution or size of costmaps if performance is critical"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"10-summary",children:"10. Summary"}),"\n",(0,a.jsx)(n.p,{children:'In this week, we covered Nav2 setup and configuration for bipedal humanoid robots. We explored how to adapt the navigation stack for the specific requirements of humanoid locomotion, including footstep planning and balance considerations. This completes Module 3 on "The AI-Robot Brain", providing you with the tools to create intelligent robots that can perceive their environment and navigate through it safely.'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"module-summary-the-ai-robot-brain-nvidia-isaac",children:"Module Summary: The AI-Robot Brain (NVIDIA Isaac\u2122)"}),"\n",(0,a.jsx)(n.p,{children:'This module has covered the NVIDIA Isaac platform, which provides the "brain" for intelligent robotics systems. We learned:'}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim"}),": High-fidelity simulation for robotics development"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"}),": Hardware-accelerated perception capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Nav2"}),": Navigation and path planning for humanoid robots"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"These technologies form the core of modern robotics AI systems, enabling robots to perceive, understand, and navigate in complex environments. The integration of these tools with ROS 2 provides a powerful and unified platform for building sophisticated humanoid robots."}),"\n",(0,a.jsx)(n.h2,{id:"quizreflection-questions",children:"Quiz/Reflection Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the main differences between Isaac Sim and traditional robotics simulators?"}),"\n",(0,a.jsx)(n.li,{children:"How does hardware acceleration in Isaac ROS improve robot perception capabilities?"}),"\n",(0,a.jsx)(n.li,{children:"What unique challenges must be addressed when adapting Nav2 for bipedal humanoid robots compared to wheeled robots?"}),"\n",(0,a.jsx)(n.li,{children:"How would you design a perception pipeline that combines Isaac ROS packages for a humanoid robot?"}),"\n",(0,a.jsx)(n.li,{children:"What factors would you consider when tuning Nav2 parameters specifically for humanoid robot navigation?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,a.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/isaac-sim.html",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/isaac-sim.html"})]}),"\n",(0,a.jsxs)(n.li,{children:["NVIDIA Isaac ROS Documentation: ",(0,a.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_common/index.html",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_common/index.html"})]}),"\n",(0,a.jsxs)(n.li,{children:["Navigation2 Documentation: ",(0,a.jsx)(n.a,{href:"https://navigation.ros.org/",children:"https://navigation.ros.org/"})]}),"\n",(0,a.jsxs)(n.li,{children:["ROS 2 Documentation: ",(0,a.jsx)(n.a,{href:"https://docs.ros.org/en/humble/index.html",children:"https://docs.ros.org/en/humble/index.html"})]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>t});var a=r(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);