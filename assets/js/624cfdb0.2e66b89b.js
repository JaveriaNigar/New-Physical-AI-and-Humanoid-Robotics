"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6348],{2642:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});var a=r(4848),i=r(8453);const s={title:"Sensor Simulation (LiDAR, Depth, IMU)",description:"Simulating various sensors in Gazebo for humanoid robotics applications",tags:["Gazebo","LiDAR","Depth Sensors","IMU","Sensor Simulation","Robotics","Humanoid"]},o="Sensor Simulation (LiDAR, Depth, IMU)",t={id:"week-6-7/sensor-simulation",title:"Sensor Simulation (LiDAR, Depth, IMU)",description:"Simulating various sensors in Gazebo for humanoid robotics applications",source:"@site/docs/week-6-7/sensor-simulation.md",sourceDirName:"week-6-7",slug:"/week-6-7/sensor-simulation",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/week-6-7/sensor-simulation",draft:!1,unlisted:!1,editUrl:"https://github.com/JaveriaNigar/New-Physical-AI-and-Humanoid-Robotics/docs/week-6-7/sensor-simulation.md",tags:[{label:"Gazebo",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/gazebo"},{label:"LiDAR",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/li-dar"},{label:"Depth Sensors",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/depth-sensors"},{label:"IMU",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/imu"},{label:"Sensor Simulation",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/sensor-simulation"},{label:"Robotics",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/robotics"},{label:"Humanoid",permalink:"/New-Physical-AI-and-Humanoid-Robotics/docs/tags/humanoid"}],version:"current",frontMatter:{title:"Sensor Simulation (LiDAR, Depth, IMU)",description:"Simulating various sensors in Gazebo for humanoid robotics applications",tags:["Gazebo","LiDAR","Depth Sensors","IMU","Sensor Simulation","Robotics","Humanoid"]}},l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"LiDAR Configuration",id:"lidar-configuration",level:3},{value:"3D LiDAR Configuration",id:"3d-lidar-configuration",level:3},{value:"LiDAR Noise Modeling",id:"lidar-noise-modeling",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"Advanced IMU with Magnetometer",id:"advanced-imu-with-magnetometer",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Sensor Data Processing",id:"sensor-data-processing",level:3},{value:"Advanced Sensor Techniques",id:"advanced-sensor-techniques",level:2},{value:"Multi-sensor Fusion",id:"multi-sensor-fusion",level:3},{value:"Sensor Calibration in Simulation",id:"sensor-calibration-in-simulation",level:3},{value:"Sensor Validation and Testing",id:"sensor-validation-and-testing",level:2},{value:"Comparing Simulated and Real Sensors",id:"comparing-simulated-and-real-sensors",level:3},{value:"Sensor Quality Metrics",id:"sensor-quality-metrics",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Quiz",id:"quiz",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"sensor-simulation-lidar-depth-imu",children:"Sensor Simulation (LiDAR, Depth, IMU)"}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Simulate various types of sensors in Gazebo for humanoid robots"}),"\n",(0,a.jsx)(e.li,{children:"Configure realistic sensor parameters and noise models"}),"\n",(0,a.jsx)(e.li,{children:"Implement LiDAR sensor simulation for environment perception"}),"\n",(0,a.jsx)(e.li,{children:"Set up depth camera simulation for 3D perception"}),"\n",(0,a.jsx)(e.li,{children:"Configure IMU simulation for orientation and acceleration measurement"}),"\n",(0,a.jsx)(e.li,{children:"Integrate simulated sensors with ROS 2 for realistic data streams"}),"\n",(0,a.jsx)(e.li,{children:"Validate sensor models against real-world sensor characteristics"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation in Gazebo plays a crucial role in the development and testing of humanoid robot perception, navigation, and control systems. By creating realistic simulations of physical sensors, developers can test their algorithms in complex virtual environments before deploying to real hardware, saving time, effort, and reducing the risk of damage to expensive robotic systems."}),"\n",(0,a.jsx)(e.p,{children:"For humanoid robots, accurate sensor simulation is particularly important due to their complex multi-modal sensing requirements. These robots typically need to perceive their environment in 3D, maintain balance and orientation using inertial sensors, and navigate through spaces designed for human activity."}),"\n",(0,a.jsx)(e.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"LiDAR (Light Detection and Ranging) sensors are essential for humanoid robots for tasks such as mapping, localization, and obstacle detection. In Gazebo, LiDAR sensors can be simulated using ray-traced sensors that accurately model beam propagation and reflections."}),"\n",(0,a.jsx)(e.h3,{id:"lidar-configuration",children:"LiDAR Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_2d" type="ray">\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>720</samples>  \x3c!-- Higher resolution for better perception --\x3e\r\n        <resolution>1</resolution>\r\n        <min_angle>-3.14159</min_angle>  \x3c!-- 180 degree FoV --\x3e\r\n        <max_angle>3.14159</max_angle>\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.1</min>  \x3c!-- Minimum range --\x3e\r\n      <max>30.0</max>  \x3c!-- Maximum range --\x3e\r\n      <resolution>0.01</resolution>  \x3c!-- Resolution of sensor --\x3e\r\n    </range>\r\n  </ray>\r\n  <plugin name="lidar_2d_controller" filename="libgazebo_ros_ray_sensor.so">\r\n    <ros>\r\n      <namespace>/lidar</namespace>\r\n      <remapping>~/out:=scan</remapping>\r\n    </ros>\r\n    <output_type>sensor_msgs/LaserScan</output_type>\r\n    <frame_name>lidar_2d_frame</frame_name>\r\n  </plugin>\r\n  <always_on>true</always_on>\r\n  <update_rate>10</update_rate>  \x3c!-- Update rate in Hz --\x3e\r\n  <visualize>true</visualize>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"3d-lidar-configuration",children:"3D LiDAR Configuration"}),"\n",(0,a.jsx)(e.p,{children:"For humanoid robots, 3D LiDAR sensors are often needed for spatial awareness:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="velodyne_vlp16" type="ray">\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>1800</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-3.14159</min_angle>\r\n        <max_angle>3.14159</max_angle>\r\n      </horizontal>\r\n      <vertical>\r\n        <samples>16</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-0.261799</min_angle>  \x3c!-- -15 degrees --\x3e\r\n        <max_angle>0.261799</max_angle>   \x3c!-- +15 degrees --\x3e\r\n      </vertical>\r\n    </scan>\r\n    <range>\r\n      <min>0.2</min>\r\n      <max>100.0</max>\r\n      <resolution>0.001</resolution>\r\n    </range>\r\n  </ray>\r\n  <plugin name="velodyne_vlp16_controller" filename="libgazebo_ros_velodyne_gpu_laser.so">\r\n    <ros>\r\n      <namespace>/laser3d</namespace>\r\n      <remapping>~/out:=scan3d</remapping>\r\n    </ros>\r\n    <topic_name>points</topic_name>\r\n    <frame_name>velodyne_vlp16_frame</frame_name>\r\n    <min_range>0.2</min_range>\r\n    <max_range>100.0</max_range>\r\n    <gaussian_noise>0.008</gaussian_noise>\r\n  </plugin>\r\n  <always_on>true</always_on>\r\n  <update_rate>20</update_rate>\r\n  <visualize>true</visualize>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"lidar-noise-modeling",children:"LiDAR Noise Modeling"}),"\n",(0,a.jsx)(e.p,{children:"Real LiDAR sensors have various noise characteristics that must be modeled:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_with_noise" type="ray">\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>1081</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-2.35619</min_angle>  \x3c!-- -135 degrees --\x3e\r\n        <max_angle>2.35619</max_angle>   \x3c!-- +135 degrees --\x3e\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.08</min>\r\n      <max>30.0</max>\r\n      <resolution>0.01</resolution>\r\n    </range>\r\n  </ray>\r\n  <plugin name="lidar_noise_controller" filename="libgazebo_ros_ray_sensor.so">\r\n    <ros>\r\n      <namespace>/front_lidar</namespace>\r\n    </ros>\r\n  </plugin>\r\n  <always_on>true</always_on>\r\n  <update_rate>10</update_rate>\r\n\r\n  \x3c!-- Add realistic noise --\x3e\r\n  <noise type="gaussian">\r\n    <mean>0.0</mean>\r\n    <stddev>0.01</stddev>  \x3c!-- 1cm noise at 10m --\x3e\r\n  </noise>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras are crucial for humanoid robots for obstacle detection, manipulation planning, and 3D reconstruction."}),"\n",(0,a.jsx)(e.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\r\n  <update_rate>30</update_rate>\r\n  <camera name="head_camera">\r\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- ~60 degrees --\x3e\r\n    <image>\r\n      <format>R8G8B8</format>  \x3c!-- Color format --\x3e\r\n      <width>640</width>\r\n      <height>480</height>\r\n    </image>\r\n    <depth_camera>\r\n      <output>depths</output>\r\n    </depth_camera>\r\n    <clip>\r\n      <near>0.1</near>   \x3c!-- Near clip plane --\x3e\r\n      <far>10.0</far>    \x3c!-- Far clip plane --\x3e\r\n    </clip>\r\n  </camera>\r\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\r\n    <ros>\r\n      <namespace>/camera</namespace>\r\n      <remapping>rgb/image_raw:=image_color</remapping>\r\n      <remapping>depth/image_raw:=image_depth</remapping>\r\n      <remapping>depth/camera_info:=camera_info</remapping>\r\n    </ros>\r\n    <camera_name>depth_camera</camera_name>\r\n    <frame_name>camera_depth_optical_frame</frame_name>\r\n    <baseline>0.2</baseline>\r\n    <distortion_k1>0.0</distortion_k1>\r\n    <distortion_k2>0.0</distortion_k2>\r\n    <distortion_k3>0.0</distortion_k3>\r\n    <distortion_t1>0.0</distortion_t1>\r\n    <distortion_t2>0.0</distortion_t2>\r\n    <point_cloud_cutoff>0.5</point_cloud_cutoff>\r\n    <point_cloud_cutoff_max>3.0</point_cloud_cutoff_max>\r\n    <Cx_prime>0</Cx_prime>\r\n    <Cx>0.5</Cx>\r\n    <Cy>0.5</Cy>\r\n    <focal_length>0</focal_length>\r\n    <hack_baseline>0</hack_baseline>\r\n  </plugin>\r\n  <always_on>true</always_on>\r\n  <visualize>true</visualize>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Stereo cameras provide depth information through triangulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="stereo_camera" type="multicamera">\r\n  <camera name="left_camera">\r\n    <horizontal_fov>1.047</horizontal_fov>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>100</far>\r\n    </clip>\r\n    \x3c!-- Add noise model --\x3e\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.007</stddev>\r\n    </noise>\r\n  </camera>\r\n\r\n  <camera name="right_camera">\r\n    <pose>0.1 0 0 0 0 0</pose>  \x3c!-- 10cm baseline --\x3e\r\n    <horizontal_fov>1.047</horizontal_fov>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>100</far>\r\n    </clip>\r\n    \x3c!-- Add noise model --\x3e\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.007</stddev>\r\n    </noise>\r\n  </camera>\r\n\r\n  <plugin name="stereo_camera_controller" filename="libgazebo_ros_multicamera.so">\r\n    <ros>\r\n      <namespace>/stereo_camera</namespace>\r\n    </ros>\r\n    <camera_name>stereo_camera</camera_name>\r\n    <image_topic_name>image_raw</image_topic_name>\r\n    <camera_info_topic_name>camera_info</camera_info_topic_name>\r\n    <frame_name>stereo_camera_frame</frame_name>\r\n    <baseline>0.1</baseline>\r\n    <distortion_k1>0.0</distortion_k1>\r\n    <distortion_k2>0.0</distortion_k2>\r\n    <distortion_k3>0.0</distortion_k3>\r\n    <distortion_t1>0.0</distortion_t1>\r\n    <distortion_t2>0.0</distortion_t2>\r\n  </plugin>\r\n  <always_on>true</always_on>\r\n  <update_rate>30</update_rate>\r\n  <visualize>true</visualize>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) are critical for humanoid robots to maintain balance and determine orientation in space."}),"\n",(0,a.jsx)(e.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\r\n  <always_on>true</always_on>\r\n  <update_rate>100</update_rate>  \x3c!-- 100Hz update rate --\x3e\r\n  <visualize>false</visualize>\r\n  <topic>imu/data</topic>\r\n\r\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\r\n    <ros>\r\n      <namespace>/imu</namespace>\r\n      <remapping>~/out:=data</remapping>\r\n    </ros>\r\n    <initial_orientation_as_reference>false</initial_orientation_as_reference>\r\n    <frame_name>imu_link</frame_name>\r\n  </plugin>\r\n\r\n  <imu>\r\n    \x3c!-- Angular velocity noise --\x3e\r\n    <angular_velocity>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s (ADIS16448 spec) --\x3e\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0012</bias_stddev>  \x3c!-- ~0.07 deg/s bias drift --\x3e\r\n        </noise>\r\n      </x>\r\n      <y>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.0017</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0012</bias_stddev>\r\n        </noise>\r\n      </y>\r\n      <z>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.0017</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0012</bias_stddev>\r\n        </noise>\r\n      </z>\r\n    </angular_velocity>\r\n\r\n    \x3c!-- Linear acceleration noise --\x3e\r\n    <linear_acceleration>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.017</stddev>  \x3c!-- ~0.0017g (ADIS16448 spec) --\x3e\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0098</bias_stddev>  \x3c!-- ~0.001g bias drift --\x3e\r\n        </noise>\r\n      </x>\r\n      <y>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.017</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0098</bias_stddev>\r\n        </noise>\r\n      </y>\r\n      <z>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.017</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0098</bias_stddev>\r\n        </noise>\r\n      </z>\r\n    </linear_acceleration>\r\n  </imu>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"advanced-imu-with-magnetometer",children:"Advanced IMU with Magnetometer"}),"\n",(0,a.jsx)(e.p,{children:"For complete attitude determination:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="ahrs_sensor" type="imu">\r\n  <always_on>true</always_on>\r\n  <update_rate>100</update_rate>\r\n  <visualize>false</visualize>\r\n  <topic>imu/data</topic>\r\n\r\n  <plugin name="ahrs_plugin" filename="libgazebo_ros_imu_sensor.so">\r\n    <ros>\r\n      <namespace>/ahrs</namespace>\r\n    </ros>\r\n    <frame_name>imu_link</frame_name>\r\n    <body_name>imu_body</body_name>\r\n  </plugin>\r\n\r\n  <imu>\r\n    \x3c!-- Gyroscope parameters --\x3e\r\n    <angular_velocity>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.001</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0005</bias_stddev>\r\n          <dynamic_bias_std>0.001</dynamic_bias_std>\r\n          <dynamic_bias_correlation_time>100</dynamic_bias_correlation_time>\r\n        </noise>\r\n      </x>\r\n      <y>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.001</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0005</bias_stddev>\r\n          <dynamic_bias_std>0.001</dynamic_bias_std>\r\n          <dynamic_bias_correlation_time>100</dynamic_bias_correlation_time>\r\n        </noise>\r\n      </y>\r\n      <z>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.001</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.0005</bias_stddev>\r\n          <dynamic_bias_std>0.001</dynamic_bias_std>\r\n          <dynamic_bias_correlation_time>100</dynamic_bias_correlation_time>\r\n        </noise>\r\n      </z>\r\n    </angular_velocity>\r\n\r\n    \x3c!-- Accelerometer parameters --\x3e\r\n    <linear_acceleration>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.005</bias_stddev>\r\n        </noise>\r\n      </x>\r\n      <y>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.005</bias_stddev>\r\n        </noise>\r\n      </y>\r\n      <z>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n          <bias_mean>0.0</bias_mean>\r\n          <bias_stddev>0.005</bias_stddev>\r\n        </noise>\r\n      </z>\r\n    </linear_acceleration>\r\n  </imu>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"sensor-data-processing",children:"Sensor Data Processing"}),"\n",(0,a.jsx)(e.p,{children:"Processing simulated sensor data in ROS 2:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, Imu\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport tf2_ros\r\n\r\nclass SensorProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_processor\')\r\n\r\n        # Initialize sensor data subscribers\r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            \'/lidar/scan\',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/depth/image_raw\',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        # CV Bridge for image processing\r\n        self.bridge = CvBridge()\r\n\r\n        # TF2 broadcaster for coordinate transforms\r\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\r\n\r\n        self.get_logger().info("Sensor processor initialized")\r\n\r\n    def lidar_callback(self, msg):\r\n        # Process LiDAR data\r\n        ranges = np.array(msg.ranges)\r\n\r\n        # Filter out invalid readings\r\n        valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\r\n\r\n        # Detect obstacles\r\n        obstacle_threshold = 0.5  # meters\r\n        obstacles = valid_ranges[valid_ranges < obstacle_threshold]\r\n\r\n        if len(obstacles) > 0:\r\n            self.get_logger().info(f"Detected {len(obstacles)} obstacles")\r\n\r\n        # Update robot\'s internal map with LiDAR data\r\n        self.update_local_map(ranges, msg.angle_min, msg.angle_increment)\r\n\r\n    def camera_callback(self, msg):\r\n        # Process depth camera data\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n            # Convert to point cloud or process depth information\r\n            height, width = cv_image.shape\r\n            valid_points = []\r\n\r\n            for u in range(0, width, 10):  # Downsample for efficiency\r\n                for v in range(0, height, 10):\r\n                    depth = cv_image[v, u]\r\n                    if depth > 0 and depth < 5.0:  # Valid depth range\r\n                        # Convert pixel coordinates to 3D point\r\n                        # Simplified assuming known camera parameters\r\n                        x = (u - width/2) * depth / 525  # focal length approx\r\n                        y = (v - height/2) * depth / 525\r\n                        z = depth\r\n                        valid_points.append([x, y, z])\r\n\r\n            self.get_logger().info(f"Processed {len(valid_points)} valid depth points")\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing camera image: {e}")\r\n\r\n    def imu_callback(self, msg):\r\n        # Process IMU data for balance/attitude control\r\n        orientation = msg.orientation\r\n        angular_velocity = msg.angular_velocity\r\n        linear_acceleration = msg.linear_acceleration\r\n\r\n        # Check for orientation changes that might indicate instability\r\n        roll, pitch, yaw = self.quaternion_to_euler(\r\n            orientation.w, orientation.x, orientation.y, orientation.z\r\n        )\r\n\r\n        # Check if robot is tilting too much\r\n        tilt_threshold = 0.5  # radians (~28 degrees)\r\n        if abs(roll) > tilt_threshold or abs(pitch) > tilt_threshold:\r\n            self.get_logger().warn(f"Robot tilt detected: roll={roll:.2f}, pitch={pitch:.2f}")\r\n\r\n        # Use IMU data for balance controller\r\n        self.update_balance_controller(roll, pitch, angular_velocity)\r\n\r\n    def quaternion_to_euler(self, w, x, y, z):\r\n        """Convert quaternion to Euler angles."""\r\n        # Roll (x-axis rotation)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\r\n\r\n        # Pitch (y-axis rotation)\r\n        sinp = 2 * (w * y - z * x)\r\n        if np.abs(sinp) >= 1:\r\n            pitch = np.copysign(np.pi / 2, sinp)\r\n        else:\r\n            pitch = np.arcsin(sinp)\r\n\r\n        # Yaw (z-axis rotation)\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\r\n\r\n        return roll, pitch, yaw\r\n\r\n    def update_local_map(self, ranges, angle_min, angle_increment):\r\n        """Update local occupancy grid based on laser scan."""\r\n        # Placeholder for mapping implementation\r\n        pass\r\n\r\n    def update_balance_controller(self, roll, pitch, angular_velocity):\r\n        """Update balance controller based on IMU data."""\r\n        # Placeholder for balance control implementation\r\n        pass\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    sensor_processor = SensorProcessor()\r\n\r\n    rclpy.spin(sensor_processor)\r\n\r\n    sensor_processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-sensor-techniques",children:"Advanced Sensor Techniques"}),"\n",(0,a.jsx)(e.h3,{id:"multi-sensor-fusion",children:"Multi-sensor Fusion"}),"\n",(0,a.jsx)(e.p,{children:"Combining data from multiple sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <sensor_msgs/msg/laser_scan.hpp>\r\n#include <sensor_msgs/msg/imu.hpp>\r\n#include <sensor_msgs/msg/image.hpp>\r\n#include <message_filters/subscriber.h>\r\n#include <message_filters/time_synchronizer.h>\r\n#include <tf2_ros/transform_listener.h>\r\n#include <Eigen/Dense>\r\n\r\nclass SensorFusionNode : public rclcpp::Node\r\n{\r\npublic:\r\n    SensorFusionNode() : Node("sensor_fusion_node")\r\n    {\r\n        // Initialize subscribers for different sensors\r\n        lidar_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\r\n            "/lidar/scan", 10,\r\n            std::bind(&SensorFusionNode::lidarCallback, this, std::placeholders::_1)\r\n        );\r\n\r\n        imu_sub_ = this->create_subscription<sensor_msgs::msg::Imu>(\r\n            "/imu/data", 10,\r\n            std::bind(&SensorFusionNode::imuCallback, this, std::placeholders::_1)\r\n        );\r\n\r\n        camera_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\r\n            "/camera/depth/image_raw", 10,\r\n            std::bind(&SensorFusionNode::cameraCallback, this, std::placeholders::_1)\r\n        );\r\n\r\n        RCLCPP_INFO(this->get_logger(), "Sensor fusion node initialized");\r\n    }\r\n\r\nprivate:\r\n    void lidarCallback(const sensor_msgs::msg::LaserScan::SharedPtr msg)\r\n    {\r\n        // Process laser scan and update particle filter or EKF\r\n        this->processLidarData(*msg);\r\n    }\r\n\r\n    void imuCallback(const sensor_msgs::msg::Imu::SharedPtr msg)\r\n    {\r\n        // Use IMU for orientation and acceleration\r\n        this->processImuData(*msg);\r\n    }\r\n\r\n    void cameraCallback(const sensor_msgs::msg::Image::SharedPtr msg)\r\n    {\r\n        // Process depth information\r\n        this->processCameraData(*msg);\r\n    }\r\n\r\n    // Methods for sensor fusion\r\n    void processLidarData(const sensor_msgs::msg::LaserScan& scan)\r\n    {\r\n        // Implement LiDAR-based localization update\r\n    }\r\n\r\n    void processImuData(const sensor_msgs::msg::Imu& imu)\r\n    {\r\n        // Implement IMU-based prediction/prior update\r\n        Eigen::Vector3d angular_velocity(\r\n            imu.angular_velocity.x,\r\n            imu.angular_velocity.y,\r\n            imu.angular_velocity.z\r\n        );\r\n\r\n        Eigen::Vector3d linear_acc(\r\n            imu.linear_acceleration.x,\r\n            imu.linear_acceleration.y,\r\n            imu.linear_acceleration.z\r\n        );\r\n\r\n        // Update state estimate based on IMU\r\n    }\r\n\r\n    void processCameraData(const sensor_msgs::msg::Image& img)\r\n    {\r\n        // Process depth image to extract obstacles\r\n    }\r\n\r\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr lidar_sub_;\r\n    rclcpp::Subscription<sensor_msgs::msg::Imu>::SharedPtr imu_sub_;\r\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr camera_sub_;\r\n};\n'})}),"\n",(0,a.jsx)(e.h3,{id:"sensor-calibration-in-simulation",children:"Sensor Calibration in Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Even in simulation, sensors may need calibration:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="calibrated_camera" type="camera">\r\n  <camera>\r\n    <horizontal_fov>1.047</horizontal_fov>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>100</far>\r\n    </clip>\r\n    \x3c!-- Distortion parameters --\x3e\r\n    <distortion>\r\n      <k1>-0.1742</k1>\r\n      <k2>0.0392</k2>\r\n      <k3>-0.0003</k3>\r\n      <p1>0.0034</p1>\r\n      <p2>-0.0023</p2>\r\n      <center>0.5 0.5</center>\r\n    </distortion>\r\n  </camera>\r\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n    <frame_name>camera_link</frame_name>\r\n    <min_depth>0.1</min_depth>\r\n    <max_depth>100.0</max_depth>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-validation-and-testing",children:"Sensor Validation and Testing"}),"\n",(0,a.jsx)(e.h3,{id:"comparing-simulated-and-real-sensors",children:"Comparing Simulated and Real Sensors"}),"\n",(0,a.jsx)(e.p,{children:"When validating that your simulation adequately represents real-world performance:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Compare noise characteristics"}),": Ensure simulated noise matches real sensor data"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Validate update rates"}),": Verify the simulation runs at rates comparable to real sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Check range limitations"}),": Ensure simulated range and FoV match real sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Test edge cases"}),": Verify behavior in challenging conditions (e.g., bright light, reflective surfaces)"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"sensor-quality-metrics",children:"Sensor Quality Metrics"}),"\n",(0,a.jsx)(e.p,{children:"Implement sensor quality metrics to evaluate performance:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy.spatial.distance import cdist\r\n\r\ndef evaluate_lidar_quality(simulated_scan, real_scan, threshold=0.1):\r\n    \"\"\"\r\n    Evaluate the quality of a simulated LiDAR scan compared to a real one.\r\n    \"\"\"\r\n    # Calculate mean absolute error\r\n    mae = np.mean(np.abs(simulated_scan - real_scan))\r\n\r\n    # Calculate root mean squared error\r\n    rmse = np.sqrt(np.mean((simulated_scan - real_scan)**2))\r\n\r\n    # Calculate hit rate (percentage of valid beams)\r\n    valid_sim = np.logical_and(simulated_scan > 0.1, simulated_scan < 30.0)\r\n    valid_real = np.logical_and(real_scan > 0.1, real_scan < 30.0)\r\n    hit_rate = np.sum(valid_sim) / len(valid_sim)\r\n\r\n    return {\r\n        'MAE': mae,\r\n        'RMSE': rmse,\r\n        'Hit_Rate': hit_rate,\r\n        'Quality_Score': 1.0 - (rmse / threshold) if rmse < threshold else 0.0\r\n    }\r\n\r\ndef evaluate_camera_quality(sim_depth, real_depth, threshold=0.05):\r\n    \"\"\"\r\n    Evaluate depth camera simulation quality.\r\n    \"\"\"\r\n    # Remove invalid depth values\r\n    valid_mask = np.logical_and(sim_depth > 0.1, real_depth > 0.1)\r\n    valid_sim = sim_depth[valid_mask]\r\n    valid_real = real_depth[valid_mask]\r\n\r\n    if len(valid_sim) == 0:\r\n        return {'Error': 'No valid depth values'}\r\n\r\n    # Calculate metrics\r\n    mae = np.mean(np.abs(valid_sim - valid_real))\r\n    rmse = np.sqrt(np.mean((valid_sim - valid_real)**2))\r\n\r\n    # Percentage of measurements within threshold\r\n    within_threshold = np.sum(np.abs(valid_sim - valid_real) < threshold) / len(valid_sim)\r\n\r\n    return {\r\n        'MAE': mae,\r\n        'RMSE': rmse,\r\n        'Within_Threshold': within_threshold,\r\n        'Pixel_Count': len(valid_sim)\r\n    }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Configure a LiDAR sensor in Gazebo with realistic noise characteristics based on a real sensor's specifications."}),"\n",(0,a.jsx)(e.li,{children:"Create a simulation of a humanoid robot with multiple sensors (LiDAR, camera, IMU) and implement a basic sensor fusion system."}),"\n",(0,a.jsx)(e.li,{children:"Add distortion parameters to a camera simulation to match those of a real RGB-D camera."}),"\n",(0,a.jsx)(e.li,{children:"Implement a validation system that compares simulated sensor data to real-world measurements."}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"quiz",children:"Quiz"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What is the primary purpose of adding noise models to simulated sensors?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A) To make the simulation look more realistic"}),"\n",(0,a.jsx)(e.li,{children:"B) To develop robust algorithms that can handle real sensor imperfections"}),"\n",(0,a.jsx)(e.li,{children:"C) To slow down the simulation"}),"\n",(0,a.jsx)(e.li,{children:"D) To increase the visual quality"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Which ROS message type is typically used for 2D LiDAR data?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A) sensor_msgs/PointCloud2"}),"\n",(0,a.jsx)(e.li,{children:"B) sensor_msgs/LaserScan"}),"\n",(0,a.jsx)(e.li,{children:"C) sensor_msgs/Range"}),"\n",(0,a.jsx)(e.li,{children:"D) geometry_msgs/Point"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What is a typical update rate for IMU sensors in humanoid robots?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A) 10Hz"}),"\n",(0,a.jsx)(e.li,{children:"B) 50Hz"}),"\n",(0,a.jsx)(e.li,{children:"C) 100Hz"}),"\n",(0,a.jsx)(e.li,{children:"D) 1000Hz"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Why is it important to match the field of view (FoV) of simulated cameras to real cameras?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A) For visual rendering quality"}),"\n",(0,a.jsx)(e.li,{children:"B) To ensure algorithms trained in simulation work properly with real sensors"}),"\n",(0,a.jsx)(e.li,{children:"C) To reduce computation time"}),"\n",(0,a.jsx)(e.li,{children:"D) For ... [truncated]"}),"\n"]}),"\n"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>t});var a=r(6540);const i={},s=a.createContext(i);function o(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);